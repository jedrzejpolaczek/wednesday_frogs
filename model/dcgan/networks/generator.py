import tensorflow
import keras
from loguru import logger


def create_generator(noise_dim: int) -> tensorflow.keras.Model:
    """
    The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). 
    Start with a Dense layer that takes this seed as input, 
    then upsample several times until you reach the desired image size of 32x32x3. 
    Warning: The tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh.

    Note: description from tensorflow GAN example.

    noise_dim (int): dimensions of your input shape. Note that your input shape has only one dimension, 
    you don't need to give it as a tuple, you give input_dimensions as a scalar number.

    return (tensorflow.keras.Model): instation of generator model
    """
    logger.debug("Generator model declaration and definition.")
    model = tensorflow.keras.Sequential()

    # We want to start from processing small image and go to bigger one. 
    # For that we will reshape input to small 4x4 image with 256 channels
    start_image = (4, 4, 256)

    logger.debug("Adding generator input layer.")
    # foundation for 4x4 image
    number_of_dense_layer_nodes = start_image[0] * start_image[1] * start_image[2]  # In other words: 4 * 4 * 256  
    model.add(tensorflow.keras.layers.Dense(number_of_dense_layer_nodes, input_dim=noise_dim))
    
    logger.debug("Adding generator hidden layers.")
    model.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))
    model.add(tensorflow.keras.layers.Reshape((4, 4, 256)))

    logger.debug("Adding generator hidden layers for upsample to 8x8.")
    model.add(tensorflow.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
    model.add(tensorflow.keras.layers.BatchNormalization())
    model.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

    logger.debug("Adding generator hidden layers for upsample to 16x16.")
    model.add(tensorflow.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
    model.add(tensorflow.keras.layers.BatchNormalization())
    model.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

    logger.debug("Adding generator hidden layers for upsample to 32x32.")
    model.add(tensorflow.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))
    model.add(tensorflow.keras.layers.BatchNormalization())
    model.add(tensorflow.keras.layers.LeakyReLU(alpha=0.2))

    logger.debug("Adding generator output layer.")
    model.add(tensorflow.keras.layers.Conv2D(3, (3,3), activation='tanh', padding='same'))

    # GENERATOR MODEL OPTIMIZATION
    # Optimization function is returning by function generator_optimizer. 
    # We will add it later when we will be putting everything together to create GAN.
    
    # GENERATOR MODEL COMPILATION
    # We will compile all models at once

    logger.debug(f"Generator network: \n {model.summary()}")

    return model


def get_generator_loss(fake_output: tensorflow.Tensor) -> tensorflow.Tensor:  # Actually EagerTensor
    """
    Calculate and return loss value for generator based on output 
    from discriminator for fake (generated by generator) discriminator output.
    
    fake output (tensorflow..Tensor): fake output values from discriminator 
    contatining list with predictions and actuall values (eg. y_pred, y_true).

    return (tensorflow.Tensor): (actually EagerTensor) tensor with value of loss for fake output.
    """
    cross_entropy = tensorflow.keras.losses.BinaryCrossentropy(from_logits=True)

    return cross_entropy(tensorflow.ones_like(fake_output), fake_output)


def get_generator_optimizer() -> tensorflow.keras.optimizers.Adam:
    """
    Return instation of optimizer Adam from tensorflow librarie.

    return (keras.optimizers.optimizer_v2.adam.Adam): Optimizer Adam for generator network.
    """
    return tensorflow.keras.optimizers.Adam(1e-4)
